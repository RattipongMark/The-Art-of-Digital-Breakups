
# from transformers import MT5ForConditionalGeneration, T5Tokenizer
from pythainlp.summarize import summarize
from pythainlp.summarize import extract_keywords
import re
from transformers import pipeline
import torch
import streamlit as st
import numpy as np
from sentence_transformers import SentenceTransformer
from transformers import pipeline
from pythainlp.tokenize import sent_tokenize
import hdbscan
from collections import Counter
from tqdm import tqdm
from sklearn.metrics.pairwise import cosine_similarity

MIN_LENGTH = 100


def clean(text: str) -> str:
    if not text:
        return ""
    
    # ‡πÅ‡∏õ‡∏•‡∏á newline/tab ‡πÄ‡∏õ‡πá‡∏ô space
    text = text.replace("\n", " ").replace("\t", " ")
    
    # ‡∏•‡∏ö‡∏™‡∏±‡∏ç‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå‡∏û‡∏¥‡πÄ‡∏®‡∏© (‡∏¢‡∏Å‡πÄ‡∏ß‡πâ‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡πÑ‡∏ó‡∏¢/‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©/‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç)
    text = re.sub(r"[^‡∏Å-‡πôa-zA-Z0-9\s]", "", text)
    
    # ‡∏•‡∏ö space ‡πÄ‡∏Å‡∏¥‡∏ô
    text = re.sub(r"\s+", " ", text)
    
    # Trim ‡∏Ç‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
    text = text.strip()
    
    return text


def summarize_conditional(text: str) -> str:
    cleaned_text = clean(text)  # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å clean ‡∏Å‡πà‡∏≠‡∏ô
    MIN_LENGTH = 100
    if len(cleaned_text) < MIN_LENGTH:
        return cleaned_text
    summary_result = summarize(cleaned_text, engine="mt5-cpe-kmutt-thai-sentence-sum")
    if isinstance(summary_result, list):
        summary_result = " ".join(summary_result)

    return summary_result



# 1. Sentence embedding model
embedder = SentenceTransformer(
    "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
)

# 2. Zero-shot model (‡∏ä‡πâ‡∏≤ ‡πÅ‡∏ï‡πà‡πÅ‡∏°‡πà‡∏ô)
classifier = pipeline(
    "zero-shot-classification",
    model="joeddav/xlm-roberta-large-xnli",
    device=0  # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ GPU ‡∏à‡∏∞‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô‡∏°‡∏≤‡∏Å
)

# cache ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏ã‡πâ‡∏≥
reason_cache = {}

reason_labels = [
    "‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏Å‡∏≤‡∏£‡∏™‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏£",
    "‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô",
    "‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏ß‡πâ‡∏ß‡∏≤‡∏á‡πÉ‡∏à‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏∂‡∏á‡∏´‡∏ß‡∏á",
    "‡∏Å‡∏≤‡∏£‡∏ô‡∏≠‡∏Å‡πÉ‡∏à‡∏´‡∏£‡∏∑‡∏≠‡∏°‡∏∑‡∏≠‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°",
    "‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ó‡∏≤‡∏á‡πÄ‡∏û‡∏®",
    "‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô",
    "‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ó‡∏≤‡∏á‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ñ‡∏£‡∏µ‡∏¢‡∏î",
    "‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏à‡∏∑‡∏î‡∏à‡∏≤‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏ö‡∏∑‡πà‡∏≠‡∏´‡∏ô‡πà‡∏≤‡∏¢",
    "‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô",
    "‡∏Ñ‡πà‡∏≤‡∏ô‡∏¥‡∏¢‡∏°‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏ä‡∏µ‡∏ß‡∏¥‡∏ï‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô",
    "‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏£‡∏±‡∏ß‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏ô‡∏£‡∏≠‡∏ö‡∏Ç‡πâ‡∏≤‡∏á‡πÅ‡∏ó‡∏£‡∏Å‡πÅ‡∏ã‡∏á",
    "‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ç‡∏±‡∏î‡πÅ‡∏¢‡πâ‡∏á‡πÑ‡∏°‡πà‡∏î‡∏µ",
    "‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÉ‡∏´‡πâ‡∏Å‡∏±‡∏ô‡πÑ‡∏°‡πà‡∏û‡∏≠",
    "‡πÑ‡∏•‡∏ü‡πå‡∏™‡πÑ‡∏ï‡∏•‡πå‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô",
    "‡∏†‡∏≤‡∏£‡∏∞‡∏á‡∏≤‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå",
    "‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏ó‡∏≤‡∏á‡∏£‡∏∞‡∏¢‡∏∞‡∏ó‡∏≤‡∏á (LDR / ‡∏≠‡∏¢‡∏π‡πà‡πÑ‡∏Å‡∏•‡∏Å‡∏±‡∏ô)",
    "‡∏ö‡∏∏‡∏Ñ‡∏•‡∏¥‡∏Å‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ô‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ",
    "‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏°‡πà‡∏™‡∏°‡πà‡∏≥‡πÄ‡∏™‡∏°‡∏≠‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏±‡∏Å",
    "‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï (‡πÅ‡∏ï‡πà‡∏á‡∏á‡∏≤‡∏ô / ‡∏°‡∏µ‡∏•‡∏π‡∏Å)",
    "‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏°‡πà‡∏°‡∏±‡πà‡∏ô‡∏Ñ‡∏á‡∏ó‡∏≤‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å‡∏™‡πà‡∏ß‡∏ô‡∏ï‡∏±‡∏ß (self-esteem)",
    "‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏à‡πâ‡∏≤‡∏Ç‡∏≠‡∏á‡∏°‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ",
    "‡∏û‡∏§‡∏ï‡∏¥‡∏Å‡∏£‡∏£‡∏°‡πÄ‡∏™‡∏û‡∏ï‡∏¥‡∏î (‡πÄ‡∏Å‡∏° ‡πÅ‡∏≠‡∏•‡∏Å‡∏≠‡∏Æ‡∏≠‡∏•‡πå ‡∏Å‡∏≤‡∏£‡∏û‡∏ô‡∏±‡∏ô ‡πÇ‡∏ó‡∏£‡∏®‡∏±‡∏û‡∏ó‡πå)",
    "‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏ß‡∏•‡∏≤‡πÅ‡∏•‡∏∞‡∏™‡∏°‡∏î‡∏∏‡∏•‡∏ä‡∏µ‡∏ß‡∏¥‡∏ï",
    "‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πà‡∏≤‡∏á‡∏î‡πâ‡∏≤‡∏ô‡∏®‡∏≤‡∏™‡∏ô‡∏≤ ‡∏ß‡∏±‡∏í‡∏ô‡∏ò‡∏£‡∏£‡∏° ‡∏´‡∏£‡∏∑‡∏≠‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏£‡∏±‡∏ß",
]

def detect_reason_batch(sentences):
    """‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡πÅ‡∏ö‡∏ö batch ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô‡∏°‡∏≤‡∏Å"""
    uncached = [s for s in sentences if s not in reason_cache]
    
    if uncached:
        results = classifier(uncached, reason_labels)
        for s, r in zip(uncached, results):
            reason_cache[s] = r["labels"][0]

    return [reason_cache[s] for s in sentences]


def cluster_reasons(text: str):

    print("\nüîé ‡∏ï‡∏±‡∏î‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ...")
    sentences = sent_tokenize(text)
    if len(sentences) < 2:
        return {"global_reason": detect_reason_batch([text])[0], "sentences": sentences}

    print("üìå ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ:", len(sentences))

    print("\nüîß ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á embedding ...")
    embeddings = embedder.encode(sentences, show_progress_bar=True)

    print("\nüîç ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ó‡∏≥ HDBSCAN clustering ...")
    clusterer = hdbscan.HDBSCAN(min_cluster_size=2, metric='euclidean')
    labels = clusterer.fit_predict(embeddings)

    # fallback ‡∏ñ‡πâ‡∏≤ HDBSCAN ‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ cluster
    if len(set(labels)) <= 1:
        print("\n‚ö†Ô∏è HDBSCAN ‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ cluster ‚Üí ‡πÉ‡∏ä‡πâ Agglomerative ‡πÅ‡∏ó‡∏ô")
        # from sklearn.cluster import AgglomerativeClustering
        # n_clusters = min(3, len(sentences))
        # clusterer2 = AgglomerativeClustering(n_clusters=n_clusters)
        # labels = clusterer2.fit_predict(embeddings)

    print("\nüìä ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ cluster ...")

    result = {}

    for label in set(labels):
        cluster_sents = [sentences[i] for i in range(len(sentences)) if labels[i] == label]
        
        reasons = detect_reason_batch(cluster_sents)
        main_reason = Counter(reasons).most_common(1)[0][0]

        result[main_reason] = cluster_sents

    return result



emotion_labels = [
    "‡∏ï‡∏•‡∏Å/‡∏õ‡∏£‡∏∞‡∏ä‡∏î",
    "‡πÄ‡∏®‡∏£‡πâ‡∏≤/‡πÄ‡∏™‡∏µ‡∏¢‡πÉ‡∏à",
    "‡πÇ‡∏Å‡∏£‡∏ò/‡∏£‡∏∞‡∏ö‡∏≤‡∏¢",
    "‡∏£‡∏∏‡∏ô‡πÅ‡∏£‡∏á/‡∏≠‡∏±‡∏ô‡∏ï‡∏£‡∏≤‡∏¢",
    "‡∏™‡∏±‡∏ö‡∏™‡∏ô",
    "‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á",
    "‡∏õ‡∏•‡∏á/‡∏¢‡∏≠‡∏°‡∏£‡∏±‡∏ö"
]

# model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
label_embs = embedder.encode(emotion_labels)

# ---------- FUNCTION ----------
def predict_emotion(text: str):

    if not isinstance(text, str) or text.strip() == "":
        return None, None

    text_emb = embedder.encode(text)
    sims = cosine_similarity([text_emb], label_embs)[0]
    best_idx = np.argmax(sims)

    return emotion_labels[best_idx], float(sims[best_idx])